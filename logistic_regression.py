# -*- coding: utf-8 -*-
"""Logistic regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FZg_FO33cTLgiMovJcUpXDYG42NB-tIO
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


import random

"""
  # Logistic Regression

- basically linear regression but with an activation function.
  
- cross entropy loss instead of mse because sigmoid -> non-linear -> multiple local minimums
  
- cross-entropy loss can be divided into two separate cost functions: one for 𝑦 = 1 and one for 𝑦 = 0

    ## Steps:
        Initialize weights to zero.
        Compute the sigmoid activation function
        Update weights using gradient descent:
        w = w − α⋅1/m X^T (σ(Xw)−y)
        Predict class labels based on probability threshold (> 0.5)

    ## Detailed steps:

      z = wX + b (same as linear reg)

      we pass it through a sigmoid fn to get probs
        y_hat = sig(wX + b)

      instead of mse we use log loss / binary cross entropy

      bernoulli's:
        P(y = 1|x) = sig(wX + b) = y_hat
        P(y = 0|x) = 1 - sig(wX + b) = 1 - log y_hat

        L = y(log y_hat) + (1 - y) log(1 - y_hat)
        J(w, b) = - 1/len(y) ∑(i: 1 --> m)[L(y_i)]

        - where y_hat is basically the predicted probability found before using sigmoid
        - y is the actual label

      Computing gradient (derivative of J wrt. w and b)
        dJ / dw = 1/len(y) X_Trans (y_hat - y)      # calculates how much each feature contributes to the error
        dJ / db = 1/len(y) ∑ (y_hat - y)            # calculates error difference

      gradient descent optim:
        w := w - lr x (1/len(y) X_Trans (y_hat - y))
        b := b - lr x (1/len(y) ∑ (y_hat - y))

"""

def sigmoid(z):
  return 1 / (1 + np.exp(-z))

class LogisticRegression():

  def fit(self, X, y, lr = 0.01, iters = 1000):

    self.w = np.zeros(X.shape[1])
    self.b = 0

    for _ in range(iters):
      z = X @ self.w + self.b

      y_hat = sigmoid(z)

      dw = 1/len(y) * X.T @ (y_hat - y)
      db = 1/len(y) * np.sum(y_hat - y)

      self.w -= lr * dw
      self.b -= lr * db

  def predict(self, X):
    z = np.dot(X, self.w) + self.b
    y_hat = sigmoid(z)  # get probabilities
    return (y_hat > 0.5).astype(int)  # convert probabilities to 0 or 1

X = np.array([[0.50], [0.75], [1.00], [1.25], [1.50], [1.75], [1.75], [2.00], [2.25], [2.50],
              [2.75], [3.00], [3.25], [3.50], [4.00], [4.25], [4.50], [4.75], [5.00], [5.50]])

y = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])

df = pd.DataFrame({'Hours_Studied': X.flatten(), 'Pass': y})

print(df)

X_test = np.array([[1.0], [3.0], [4.5], [8.5]])  # Test data

model = LogisticRegression()

model.fit(X, y)
model.predict(X_test)

X_range = np.linspace(0, 6, 100).reshape(-1, 1)
y_probs = sigmoid(np.dot(X_range, model.w) + model.b)

plt.scatter(X, y, color='red', label="Actual data (Pass=1, Fail=0)")
plt.plot(X_range, y_probs, label="Sigmoid Curve", color='blue')
plt.axhline(0.5, linestyle="dashed", color="green", label="Decision Boundary")
plt.xlabel("Hours Studied")
plt.ylabel("Pass Probability")
plt.legend()
plt.show()

